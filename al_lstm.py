# -*- coding: utf-8 -*-
"""al_lstm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oF1k94liunFTXjW20wkAY6XeoiFT8kOw
"""

import matplotlib.pyplot as plt
import numpy as np
from numpy import linalg as LA
import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
import csv
import random
import math
from sklearn.model_selection import train_test_split
from numpy.random import choice
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
import time
import pickle
import json
import time
from itertools import chain
import copy

from torchtext.legacy import data

SEED = 0

torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True

TEXT = data.Field(tokenize = 'spacy',
                  tokenizer_language = 'en_core_web_sm',
                  include_lengths = True)

LABEL = data.LabelField(dtype = torch.float)

BATCH_SIZE = 64
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# import pandas as pd
# import seaborn as sns 
# from scipy import stats

# test_data=pd.read_csv('IMDB.test.csv')
# print(test_data.head())
# train_data=pd.read_csv('IMDB.train.csv')
# print(train_data.head())
# train_data["sentiment"].value_counts()
from torchtext.legacy import datasets
tra_data, te_data = datasets.IMDB.splits(TEXT, LABEL)
tra_data, vad_data = tra_data.split(random_state = random.seed(SEED), split_ratio= .05)
tra_data, vad_data = tra_data.split(random_state = random.seed(SEED), split_ratio= .8)

class RNN(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, 
                 bidirectional, dropout, pad_idx):
        
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)
        
        self.rnn = nn.LSTM(embedding_dim, 
                           hidden_dim, 
                           num_layers=n_layers, 
                           bidirectional=bidirectional, 
                           dropout=dropout)
        
        self.fc = nn.Linear(hidden_dim * 2, output_dim)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, text, text_lengths):
        
        embedded = self.dropout(self.embedding(text))
        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))
        
        packed_output, (hidden, cell) = self.rnn(packed_embedded)
        
        #unpack sequence
        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)
        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))
       
        return self.fc(hidden)

class FastText(nn.Module):
    def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx):
        
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)
        
        self.fc = nn.Linear(embedding_dim, output_dim)
        
    def forward(self, text):
        
        #text = [sent len, batch size]
        
        embedded = self.embedding(text)
                
        #embedded = [sent len, batch size, emb dim]
        
        embedded = embedded.permute(1, 0, 2)
        
        #embedded = [batch size, sent len, emb dim]
        
        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) 
        
        #pooled = [batch size, embedding_dim]
                
        return self.fc(pooled)

def step_one(input_dat, device, TEXT, LABEL,
            EMBEDDING_DIM, 
            HIDDEN_DIM, 
            OUTPUT_DIM, 
            N_LAYERS, 
            BIDIRECTIONAL, 
            DROPOUT,
            N_EPOCHS, 
            BATCH_SIZE):

    train_examples = [vars(t) for t in input_dat]
    new_y = []
    for i in range(len(input_dat)):
        # preprocess the data
        if i != 0:
            dat = train_examples[:i] + train_examples[i+1 :]    
        else:
            dat = train_examples[i+1 :]
        
        test_examples = train_examples[i]
        
        with open('train.json', 'w+') as f:
            for example in dat:
                json.dump(example, f)
                f.write('\n')     

        with open('test.json', 'w+') as f:
            json.dump(test_examples, f)
            f.write('\n')
        fields = {'text': ('text', TEXT), 'label': ('label', LABEL)}

        train_data, valid_data = data.TabularDataset.splits(
            path = '.',
            train = 'train.json',
            test = 'test.json',
            format = 'json',
            fields = fields
        )

        MAX_VOCAB_SIZE = 25_000
        TEXT.build_vocab(train_data, 
                        max_size = MAX_VOCAB_SIZE, 
                        vectors = "glove.6B.100d", 
                        unk_init = torch.Tensor.normal_)
        LABEL.build_vocab(train_data)

        train_iterator, valid_iterator = data.BucketIterator.splits(
                                          (train_data, valid_data), 
                                          batch_size = BATCH_SIZE,
                                          sort_key = lambda x: len(x.text),
                                          sort_within_batch = True,
                                          device = device)
        PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]
        INPUT_DIM = len(TEXT.vocab)

        model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, 
            N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)
        # model = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)
        pretrained_embeddings = TEXT.vocab.vectors
        model.embedding.weight.data.copy_(pretrained_embeddings)

        UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]

        model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)
        model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)

        optimizer = optim.Adam(model.parameters())
        criterion = nn.BCEWithLogitsLoss()
        model = model.to(device)
        criterion = criterion.to(device)

        for epoch in range(N_EPOCHS):
            train_loss = trainning(model, train_iterator, optimizer, criterion, device)
      
        _, _, resid, _ = evaluate(model, valid_iterator, criterion, device)

        new_y.append(resid)

    return new_y

def find_data(tra_data, vad_data, resid, B2, device, TEXT, LABEL,
              EMBEDDING_DIM, 
              HIDDEN_DIM, 
              OUTPUT_DIM, 
              N_LAYERS, 
              BIDIRECTIONAL, 
              DROPOUT,
              N_EPOCHS1, BATCH_SIZE):
    train_examples = [vars(t) for t in tra_data]
    test_examples = [vars(t) for t in vad_data]

    train_examples1 = copy.deepcopy(train_examples)
    test_examples1 = copy.deepcopy(test_examples)

    fields = {'text': ('text', TEXT), 'label': ('label', LABEL)}
    ## a trick to mark the dataset
    i = 0
    with open('new_train.json', 'w+') as f:
        for example in train_examples1:
            example['label'] = i
            i += 1
            json.dump(example, f)
            f.write('\n')
    i = 0
    with open('new_test.json', 'w+') as f:
        for example in test_examples1:
            example['label'] = i
            i += 1
            json.dump(example, f)
            f.write('\n')

    new_train_data, new_text = data.TabularDataset.splits(
        path = '.',
        train = 'new_train.json',
        test = 'new_test.json',
        format = 'json',
        fields = fields
    )

    MAX_VOCAB_SIZE = 25_000

    TEXT.build_vocab(new_train_data, 
                    max_size = MAX_VOCAB_SIZE, 
                    vectors = "glove.6B.100d", 
                    unk_init = torch.Tensor.normal_)

    LABEL.build_vocab(new_train_data)

    new_train_iterator, new_valid_iterator = data.BucketIterator.splits(
        (new_train_data, new_text), 
        batch_size = BATCH_SIZE,
        sort_key = lambda x: len(x.text),
        sort_within_batch = True,
        device = device)
    
    # now prepare the data for training
    TEXT2 = data.Field(tokenize = 'spacy',
                  tokenizer_language = 'en_core_web_sm',
                  include_lengths = True)
    LABEL2 = data.LabelField(dtype = torch.float)
    fields = {'text': ('text', TEXT2), 'label': ('label', LABEL2)}

    with open('train.json', 'w+') as f:
        for example in train_examples:
            json.dump(example, f)
            f.write('\n')
            
    original_y = []
    with open('test.json', 'w+') as f:
        for example in test_examples:
            original_y.append(example['label'])
            json.dump(example, f)
            f.write('\n')

    train_data, valid_data = data.TabularDataset.splits(
        path = '.',
        train = 'train.json',
        test = 'test.json',
        format = 'json',
        fields = fields
    )

    TEXT2.build_vocab(train_data, 
                    max_size = MAX_VOCAB_SIZE, 
                    vectors = "glove.6B.100d", 
                    unk_init = torch.Tensor.normal_)

    LABEL2.build_vocab(train_data)

    train_iterator, valid_iterator = data.BucketIterator.splits(
        (train_data, valid_data), 
        batch_size = BATCH_SIZE,
        sort_key = lambda x: len(x.text),
        sort_within_batch = True,
        device = device)
    
    PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]
    INPUT_DIM = len(TEXT.vocab)
    # training
    model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, 
              BIDIRECTIONAL, DROPOUT, PAD_IDX)
    # model = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)
    pretrained_embeddings = TEXT.vocab.vectors
    model.embedding.weight.data.copy_(pretrained_embeddings)
    UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]

    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)
    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)
    optimizer = optim.Adam(model.parameters())
    criterion = nn.BCEWithLogitsLoss()
    model = model.to(device)
    criterion = criterion.to(device)
    original_y = [1 if wd == "pos" else 0 for wd in original_y]

    for epoch in range(N_EPOCHS1):

        train_loss = trainning(model, new_train_iterator, optimizer, 
                                          criterion, device, residual=resid)
       
    _, _, weight, orders = evaluate(model, new_valid_iterator, criterion, device, ori_y = original_y)
    orders = [int(item) for sublist in orders for item in sublist]
    weights = []
    for tensor in weight:
        weights.append(tensor.detach().cpu())

    weights = list(chain.from_iterable(weights))
    weights = np.array(weights)
    weights  = weights/weights.sum() # weights are in the order of shuffled data
    # order weights by the original index
    weights = [weights[i] for i in orders]

    draw = choice(range(len(weights)), B2, replace=False, p = np.array(weights))

    # subset directly from the original raw data
    new_trains = [test_examples[i] for i in draw]
    new_trains = train_examples + new_trains
    complement = np.setdiff1d(range(len(test_examples)), draw)
    new_left = [test_examples[i] for i in complement]

    # remake the data and train again
    with open('new_train.json', 'w+') as f:
        for example in new_trains:
            json.dump(example, f)
            f.write('\n')
    i = 0
    with open('new_test.json', 'w+') as f:
        for example in new_left:
            json.dump(example, f)
            f.write('\n')

def compute_loss2(test_data, device, TEXT, LABEL, N_EPOCHS,
                  EMBEDDING_DIM, 
                  HIDDEN_DIM, 
                  OUTPUT_DIM, 
                  N_LAYERS, 
                  BIDIRECTIONAL, 
                  DROPOUT, 
                  BATCH_SIZE):

    fields = {'text': ('text', TEXT), 'label': ('label', LABEL)}
    train_data, valid_data = data.TabularDataset.splits(
        path = '.',
        train = 'new_train.json',
        test = 'new_test.json',
        format = 'json',
        fields = fields
    )

    MAX_VOCAB_SIZE = 25_000
    TEXT.build_vocab(train_data, 
                    max_size = MAX_VOCAB_SIZE, 
                    vectors = "glove.6B.100d", 
                    unk_init = torch.Tensor.normal_)

    LABEL.build_vocab(train_data)

    train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(
        (train_data, valid_data, test_data), 
        batch_size = BATCH_SIZE,
        sort_key = lambda x: len(x.text),
        sort_within_batch = True,
        device = device)
    PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]
    INPUT_DIM = len(TEXT.vocab)
    print(INPUT_DIM)
    # training
    model = RNN(INPUT_DIM, 
            EMBEDDING_DIM, 
            HIDDEN_DIM, 
            OUTPUT_DIM, 
            N_LAYERS, 
            BIDIRECTIONAL, 
            DROPOUT, 
            PAD_IDX)
    # model = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)
    pretrained_embeddings = TEXT.vocab.vectors
    
    model.embedding.weight.data.copy_(pretrained_embeddings)
    UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]

    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)
    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)
    optimizer = optim.Adam(model.parameters())
    criterion = nn.BCEWithLogitsLoss()
    model = model.to(device)
    criterion = criterion.to(device)
    for epoch in range(N_EPOCHS):

        train_loss = trainning(model, train_iterator, optimizer, 
                                          criterion, device)
        print(f'Epoch: {epoch+1:02}')
        print(f'\tTrain Loss: {train_loss:.3f} ')
    eva_loss, eva_acc, _, _ = evaluate(model, valid_iterator, criterion, device)
    test_loss, test_acc, _, _ = evaluate(model, test_iterator, criterion, device)

    return eva_loss, eva_acc, train_loss, test_loss, test_acc

def compute_loss(device, TEXT, LABEL, N_EPOCHS,
                  EMBEDDING_DIM, 
                  HIDDEN_DIM, 
                  OUTPUT_DIM, 
                  N_LAYERS, 
                  BIDIRECTIONAL, 
                  DROPOUT, 
                  BATCH_SIZE):

    fields = {'text': ('text', TEXT), 'label': ('label', LABEL)}
    train_data, valid_data = data.TabularDataset.splits(
        path = '.',
        train = 'new_train.json',
        test = 'new_test.json',
        format = 'json',
        fields = fields
    )

    MAX_VOCAB_SIZE = 25_000
    TEXT.build_vocab(train_data, 
                    max_size = MAX_VOCAB_SIZE, 
                    vectors = "glove.6B.100d", 
                    unk_init = torch.Tensor.normal_)

    LABEL.build_vocab(train_data)

    train_iterator, valid_iterator = data.BucketIterator.splits(
        (train_data, valid_data), 
        batch_size = BATCH_SIZE,
        sort_key = lambda x: len(x.text),
        sort_within_batch = True,
        device = device)
    PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]
    INPUT_DIM = len(TEXT.vocab)
    
    # training
    model = RNN(INPUT_DIM, 
            EMBEDDING_DIM, 
            HIDDEN_DIM, 
            OUTPUT_DIM, 
            N_LAYERS, 
            BIDIRECTIONAL, 
            DROPOUT, 
            PAD_IDX)
    # model = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)

    pretrained_embeddings = TEXT.vocab.vectors
    
    model.embedding.weight.data.copy_(pretrained_embeddings)
    UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]

    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)
    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)
    optimizer = optim.Adam(model.parameters())
    criterion = nn.BCEWithLogitsLoss()
    model = model.to(device)
    criterion = criterion.to(device)
    for epoch in range(N_EPOCHS):

        train_loss = trainning(model, train_iterator, optimizer, 
                                          criterion, device)
        print(f'Epoch: {epoch+1:02}')
        print(f'\tTrain Loss: {train_loss:.3f} ')
    eva_loss, eva_acc, _, _ = evaluate(model, valid_iterator, criterion, device)

    return eva_loss, eva_acc, train_loss

def evaluate(model, iterator, criterion, device, ori_y = None):
    
    epoch_loss = 0
    epoch_acc = 0
    
    model.eval()
    new_y = []
    order = []
    with torch.no_grad():

        for batch in iterator:
            
            text, text_lengths = batch.text
            
            predictions = model(text, text_lengths).squeeze(1)
            # predictions = model(text).squeeze(1)
            if (ori_y is not None):
                order.append(batch.label.tolist())
                ids = np.array(batch.label.tolist(), dtype=int)
                lab = [ori_y[i] for i in ids]
                labels = torch.FloatTensor(lab).to(device)
            else:
                labels = batch.label
            
            loss = criterion(predictions, labels)
            
            acc, pred = binary_accuracy(predictions, labels)
            
            epoch_loss += loss.item()
            epoch_acc += acc.item()
            new_y.append(pred)
        
    return epoch_loss / len(iterator), epoch_acc / len(iterator), new_y, order

def binary_accuracy(preds, y):
    """
    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8
    """

    #round predictions to the closest integer
    probs = torch.sigmoid(preds)
    rounded_preds = torch.round(probs)
    correct = (rounded_preds == y).float() #convert into float for division 
    acc = correct.sum() / len(correct)
    
    # new y
    resid = torch.abs(probs - y) 
    return acc, resid

def trainning(model, iterator, optimizer, criterion, device, residual = None):
    
    epoch_loss = 0
    epoch_acc = 0
    
    model.train()
    
    for batch in iterator:
        
        optimizer.zero_grad()
        
        text, text_lengths = batch.text
        
        predictions = model(text, text_lengths).squeeze(1)
        # predictions = model(text).squeeze(1)
       
        if(residual is not None):
            ids = np.array(batch.label.tolist(), dtype=int)
            lab = [residual[i] for i in ids]
            if len(lab) == 1:
              lab = [item for sublist in lab for item in sublist]
            labels = torch.FloatTensor(lab).to(device)
            
            #acc, _ = binary_accuracy(predictions, labels)
        else:
            labels = batch.label
        
        loss = criterion(predictions, labels)
        
        loss.backward()
        
        optimizer.step()
        
        epoch_loss += loss.item()
        #epoch_acc += acc.item()
    
    return epoch_loss / len(iterator)

def ac_lstm(all_data, device, TEXT, LABEL, EMBEDDING_DIM, BATCH_SIZE,
            HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, 
            N_EPOCHS1, N_EPOCHS, SEED, prop = 0.5, B0_p = 0.5, iteration = 1):

    eval_loss = []
    eval_acc = []
    trainning_loss = []
    timing = []
    tst_loss = []
    tst_acc = []

    for i in range(iteration):
        start_time = time.time()
        tra_data, test_data = all_data.split(random_state = random.seed(SEED), split_ratio= 0.8)
        train_data, vadid_data = tra_data.split(random_state = random.seed(SEED), split_ratio= prop)
        new_y = step_one(train_data, device, TEXT, LABEL, EMBEDDING_DIM, 
                HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, 
                N_EPOCHS1, BATCH_SIZE)
        B2 = int(math.ceil(len(tra_data.examples) * prop * B0_p))
        find_data(train_data, vadid_data, new_y, B2, device, TEXT, LABEL, EMBEDDING_DIM, 
                HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, 
                N_EPOCHS1, BATCH_SIZE)

        # eva_loss, eva_acc, train_loss = compute_loss(device, TEXT, LABEL, N_EPOCHS,
        #                   EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, 
        #                   N_LAYERS, BIDIRECTIONAL, DROPOUT, BATCH_SIZE)
        eva_loss, eva_acc, train_loss, test_loss, test_acc = compute_loss2(test_data, device, TEXT, LABEL, N_EPOCHS,
                          EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, 
                          N_LAYERS, BIDIRECTIONAL, DROPOUT, BATCH_SIZE)
        end_time = time.time()
        elapsed_time = end_time - start_time
        timing.append(elapsed_time)
        eval_loss.append(eva_loss)
        eval_acc.append(eva_acc)
        trainning_loss.append(train_loss)
        tst_loss.append(test_loss)
        tst_acc.append(test_acc)

    return eval_loss, eval_acc, trainning_loss, timing, tst_loss, tst_acc

pickle.dump(eval_loss, open( "al.eval.loss.p", "wb"))
pickle.dump(eval_acc, open( "al.eval.acc.p", "wb"))
pickle.dump(trainning_loss, open( "al.train.loss.p", "wb"))
pickle.dump(timing, open("al.time.p", "wb"))
pickle.dump(tst_loss, open( "al.tst.loss.p", "wb"))
pickle.dump(tst_acc, open( "al.tst.acc.p", "wb"))

def rand_lstm(all_data, device, TEXT, LABEL, EMBEDDING_DIM, 
            HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, 
            N_EPOCHS, SEED, BATCH_SIZE, prop = 0.5, B0_p = 0.5, iteration = 1):

    eval_loss = []
    eval_acc = []
    trainning_loss = []
    timing = []
    tst_loss = []
    tst_acc = []

    for i in range(iteration):
        start_time = time.time()
        tra_data, test_data = all_data.split(random_state = random.seed(SEED), split_ratio= 0.8)
        t = prop * B0_p + prop
        train_data, vadid_data = tra_data.split(random_state = random.seed(SEED), split_ratio= t)
        train_examples = [vars(t) for t in train_data]
        test_examples = [vars(t) for t in vadid_data]

        fields = {'text': ('text', TEXT), 'label': ('label', LABEL)}

        with open('new_train.json', 'w+') as f:
            for example in train_examples:
                json.dump(example, f)
                f.write('\n')

        with open('new_test.json', 'w+') as f:
            for example in test_examples:
                json.dump(example, f)
                f.write('\n')
        eva_loss, eva_acc, train_loss, test_loss, test_acc = compute_loss2(test_data, device, TEXT, LABEL, N_EPOCHS,
                          EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, 
                          N_LAYERS, BIDIRECTIONAL, DROPOUT, BATCH_SIZE)
        end_time = time.time()
        elapsed_time = end_time - start_time
        timing.append(elapsed_time)
        eval_loss.append(eva_loss)
        eval_acc.append(eva_acc)
        trainning_loss.append(train_loss)
        tst_loss.append(test_loss)
        tst_acc.append(test_acc)

    return eval_loss, eval_acc, trainning_loss, timing, tst_loss, tst_acc

EMBEDDING_DIM = 100
HIDDEN_DIM = 256
OUTPUT_DIM = 1
N_LAYERS = 2
BIDIRECTIONAL = True
DROPOUT = 0.5
N_EPOCHS = 50

eval_loss, eval_acc, trainning_loss, timing, tst_loss, tst_acc = rand_lstm(tra_data, device, TEXT, LABEL, 
            EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, 
            N_EPOCHS, SEED, BATCH_SIZE, prop = 0.5, B0_p = 0.5, iteration = 25)

pickle.dump(eval_loss, open( "rand.eval.loss.p", "wb"))
pickle.dump(eval_acc, open( "rand.eval.acc.p", "wb"))
pickle.dump(trainning_loss, open( "rand.train.loss.p", "wb"))
pickle.dump(timing, open("rand.time.p", "wb"))
pickle.dump(tst_loss, open( "rand.tst.loss.p", "wb"))
pickle.dump(tst_acc, open( "rand.tst.acc.p", "wb"))

f = open("al.eval.loss.p", 'rb')
rand_eva_loss = pickle.load(f)
f.close()  
f = open("al.eval.acc.p", 'rb')
rand_eva_acc = pickle.load(f)
f.close() 
f = open("al.tst.loss.p", 'rb')
rand_tst_loss = pickle.load(f)
f.close()  
f = open("al.tst.acc.p", 'rb')
rand_tst_acc = pickle.load(f)
f.close()   
f = open("al.train.loss.p", 'rb')
rand_train_loss = pickle.load(f)
f.close()

f = open("rand.eval.loss.p", 'rb')
rand_eva_loss = pickle.load(f)
f.close()  
f = open("rand.eval.acc.p", 'rb')
rand_eva_acc = pickle.load(f)
f.close() 
f = open("rand.tst.loss.p", 'rb')
rand_tst_loss = pickle.load(f)
f.close()  
f = open("rand.tst.acc.p", 'rb')
rand_tst_acc = pickle.load(f)
f.close()   
f = open("rand.train.loss.p", 'rb')
rand_train_loss = pickle.load(f)
f.close()

import seaborn as sns
import pandas as pd

da = {'Method': np.repeat(np.array(['OOB', 'Random']), 
          [len(trainning_loss), len(rand_eva_loss)], axis=0),
        'train.loss': trainning_loss + rand_train_loss,
        'valid.loss': eval_loss+ rand_eva_loss,
        'test.loss': tst_loss+ rand_tst_loss,
        'valid.acc': eval_acc+ rand_eva_acc,
        'test.acc': tst_acc+ rand_tst_acc,
        }
 
# Convert the dictionary into DataFrame 
df = pd.DataFrame(da)

df = df.reset_index()
d = pd.melt(df, id_vars='Method', value_vars=['train.loss', 'valid.loss', 
                                       'test.loss', 'valid.acc', 
                                       'test.acc'])

d.to_csv(r'res.csv', index = False, header=True)

g = sns.catplot(x="Method", 
                y="value",
                col="variable", 
                aspect=0.5,
                dodge=False,
                kind="box",
                data=d);
g.set();

def bootstrap(limit, b, k, seed):
    """
    out_id: data id for training in each bst sample
    ##hash_tb: data id -> bst sample that not in
    """
    full = np.array(range(limit))
    np.random.seed(seed)
    hash_tb = dict()
    out_id = []
    for i in range(k):
        pickups = np.array(np.random.randint(0, limit, size = b))
        out_id.append(pickups)
     
    return out_id

def boot_lstm(input_dat, TEXT, LABEL, N_EPOCHS, EMBEDDING_DIM, HIDDEN_DIM, 
              OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, BATCH_SIZE,
              seed, device, k = 50):
    train_examples = [vars(t) for t in input_dat]
    new_y = []
    limit = len(input_dat)
    full = np.array(range(limit))
    out_id = bootstrap(limit, limit, k, seed)
    pool_pred = [None] * limit

    all_y = []
    for example in train_examples:
        if (example['label'] == 'neg'):
            all_y.append(0)
        else:
            all_y.append(1)

    for i in range(k):
        dat = []
        test_examples = []
        ids = out_id[i]
        complement = np.setdiff1d(full, ids)
        for j in range(len(ids)):
          dat.append(train_examples[j])
        for j in range(len(complement)):
          test_examples.append(train_examples[j])
        
        with open('train.json', 'w+') as f:
            for example in dat:
                json.dump(example, f)
                f.write('\n')     

        original_y = []
        with open('test.json', 'w+') as f:
          for example in test_examples:
              original_y.append(example['label'])
              json.dump(example, f)
              f.write('\n')

        fields = {'text': ('text', TEXT), 'label': ('label', LABEL)}

        train_data, valid_data = data.TabularDataset.splits(
            path = '.',
            train = 'train.json',
            test = 'test.json',
            format = 'json',
            fields = fields
        )

        MAX_VOCAB_SIZE = 25_000
        TEXT.build_vocab(train_data, 
                        max_size = MAX_VOCAB_SIZE, 
                        vectors = "glove.6B.100d", 
                        unk_init = torch.Tensor.normal_)
        LABEL.build_vocab(train_data)

        train_iterator, valid_iterator = data.BucketIterator.splits(
                                  (train_data, valid_data), 
                                  batch_size = BATCH_SIZE,
                                  sort_key = lambda x: len(x.text),
                                  sort_within_batch = True,
                                  device = device)
        
        tmp, valid_iterator = data.BucketIterator.splits(
                                          (train_data, valid_data), 
                                          batch_size = len(valid_data),
                                          sort_key = lambda x: len(x.text),
                                          sort_within_batch = True,
                                          device = device)
       
        PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]
        INPUT_DIM = len(TEXT.vocab)

        model = RNN(INPUT_DIM, 
            EMBEDDING_DIM, 
            HIDDEN_DIM, 
            OUTPUT_DIM, 
            N_LAYERS, 
            BIDIRECTIONAL, 
            DROPOUT, 
            PAD_IDX)
        
        # model = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)
        pretrained_embeddings = TEXT.vocab.vectors
        model.embedding.weight.data.copy_(pretrained_embeddings)

        UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]

        model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)
        model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)

        optimizer = optim.Adam(model.parameters())
        criterion = nn.BCEWithLogitsLoss()
        model = model.to(device)
        criterion = criterion.to(device)
        
        for epoch in range(N_EPOCHS):
            train_loss = trainning(model, train_iterator, optimizer, criterion, device)
          
            print(f'Epoch: {epoch+1:02}')
            print(f'\tTrain Loss: {train_loss:.3f}%')

        original_y = [1 if wd == "pos" else 0 for wd in original_y]
        _, _, resid, orders = evaluate(model, valid_iterator, criterion, device, 
                                  ori_y = original_y)
        orders = [int(item) for sublist in orders for item in sublist]
        # order resid by the original index
        resid = resid[0]
        resid = [resid[i] for i in orders]

        for i in range(len(complement)):
            if pool_pred[complement[i]] is None:
                pool_pred[complement[i]] = []
            pool_pred[complement[i]].append(resid[i].detach().to('cpu'))
    
    i = 0
    resid = []
    ## average across the pred prob
    for pred in pool_pred:
        if pred is None:
            resid.append(all_y[i])
            i += 1
            continue
        pred = np.array(pred)
        pread_mean = np.mean(pred, axis=0)
        # normalize the estimation
        
        res = abs(all_y[i] - pread_mean)
        resid.append(res)
        i += 1

    new_y = []
    for res in resid:
        new_y.append(LA.norm(res))
        
    return new_y

#### active learning with bootstrap sampling
def bt_al(all_data, device, TEXT, LABEL, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, 
          N_LAYERS, BIDIRECTIONAL, DROPOUT, N_EPOCHS1,
          N_EPOCHS, SEED, BATCH_SIZE, m, train_size = 0.8, prop = 0.5, B0_p = 0.5,
          bt_samples = 50, iteration = 1):
    
    eval_loss = []
    eval_acc = []
    trainning_loss = []
    timing = []
    tst_loss = []
    tst_acc = []

    for i in range(iteration):

      start_time = time.time()
      tra_data, test_data = all_data.split(random_state = random.seed(SEED), 
                                           split_ratio= train_size)
      # - random sample B2 from n traning points
      train_data, vadid_data = tra_data.split(random_state = random.seed(SEED), 
                                              split_ratio= prop)
      
      b = math.ceil(len(tra_data) * prop * B0_p / m)

      # Use B2 for bootsrtap sampling, then get new y, then predict on n-B2, select b; then B2 + b for training, 
      # sampling, then repeat, until m times

      for i in range(m):
          new_y = boot_lstm(train_data, TEXT, LABEL, N_EPOCHS1, EMBEDDING_DIM, HIDDEN_DIM, 
              OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, BATCH_SIZE,
              SEED, device, k = bt_samples)
          new_y = np.array(new_y)
          
          
          find_data(train_data, vadid_data, new_y, b, device, TEXT, LABEL, EMBEDDING_DIM, 
                HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, 
                N_EPOCHS1, BATCH_SIZE)
          
          fields = {'text': ('text', TEXT), 'label': ('label', LABEL)}
          train_data, valid_data = data.TabularDataset.splits(
              path = '.',
              train = 'new_train.json',
              test = 'new_test.json',
              format = 'json',
              fields = fields
          )

          MAX_VOCAB_SIZE = 25_000
          TEXT.build_vocab(train_data, 
                          max_size = MAX_VOCAB_SIZE, 
                          vectors = "glove.6B.100d", 
                          unk_init = torch.Tensor.normal_)

          LABEL.build_vocab(train_data)
          
      ## convert new_train.json and new_test.json to train_data, vadid_data at the next stage

      eva_loss, eva_acc, train_loss, test_loss, test_acc = compute_loss2(test_data, 
                          device, TEXT, LABEL, N_EPOCHS,
                          EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, 
                          N_LAYERS, BIDIRECTIONAL, DROPOUT, BATCH_SIZE)

      end_time = time.time()
      elapsed_time = end_time - start_time
      timing.append(elapsed_time)
      eval_loss.append(eva_loss)
      eval_acc.append(eva_acc)
      trainning_loss.append(train_loss)
      tst_loss.append(test_loss)
      tst_acc.append(test_acc)
                    
    return eval_loss, eval_acc, trainning_loss, timing, tst_loss, tst_acc

EMBEDDING_DIM = 100
HIDDEN_DIM = 256
OUTPUT_DIM = 1
N_LAYERS = 2
BIDIRECTIONAL = True
DROPOUT = 0.5
N_EPOCHS = 50
N_EPOCHS1 = 10

eval_loss, eval_acc, trainning_loss, timing, tst_loss, tst_acc = bt_al(tra_data,
                    device, TEXT, LABEL, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, 
                    N_LAYERS, BIDIRECTIONAL, DROPOUT, N_EPOCHS1, N_EPOCHS, SEED, BATCH_SIZE, 
                    m = 3, train_size = 0.8, prop = 0.5, B0_p = 0.5,
                    bt_samples = 100, iteration = 1)

pickle.dump(eval_loss, open( "al.eval.loss.p", "wb"))
pickle.dump(eval_acc, open( "al.eval.acc.p", "wb"))
pickle.dump(trainning_loss, open( "al.train.loss.p", "wb"))
pickle.dump(timing, open("al.time.p", "wb"))
pickle.dump(tst_loss, open( "al.tst.loss.p", "wb"))
pickle.dump(tst_acc, open( "al.tst.acc.p", "wb"))