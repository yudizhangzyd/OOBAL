# -*- coding: utf-8 -*-
"""al.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qbz8apEhsshgS_ueQh3fkZaFglbxC6Lb
"""

import matplotlib.pyplot as plt
import numpy as np
from numpy import linalg as LA
import torch
import torch.nn as nn
import torch.nn.functional as F
import csv
import random
import math
from sklearn.model_selection import train_test_split
from numpy.random import choice
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder

# - use B2(B/2)-1 to train the model and predict the left one (LOOCV)
class ConvNet(nn.Module):
    def __init__(self, out_size = 16, out_size2 = 16, num_classes = 2):
        super(ConvNet, self).__init__()
        
        self.layer1 = nn.Sequential(
            nn.Conv1d(1, out_size, kernel_size=5, stride=1),
            nn.BatchNorm1d(out_size),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2, stride=2))
        self.layer2 = nn.Sequential(
            nn.Conv1d(out_size, out_size2, kernel_size=5),
            nn.BatchNorm1d(out_size2),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2, stride=2))
        self.fc = nn.Linear(4*out_size2, num_classes)
        
    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = out.reshape(out.size(0), -1)
        out = self.fc(out)
        return out

def random_mini_batches(X, Y, mini_batch_size, seed = 0):
    """
    Creates a list of random minibatches from (X, Y)
    
    Arguments:
    X -- input data, of shape (input size, number of examples)
    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)
    mini_batch_size -- size of the mini-batches, integer
    
    Returns:
    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)
    """
    
    np.random.seed(seed)            # To make your "random" minibatches the same as ours
    m = X.shape[0]                  # number of training examples
    mini_batches = []
        
    # Step 1: Shuffle (X, Y)
    permutation = list(np.random.permutation(m))
    shuffled_X = X[permutation, :]
    shuffled_Y = Y[permutation]
    
    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.
    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning
    for k in range(0, num_complete_minibatches):
        mini_batch_X = shuffled_X[k * mini_batch_size : (k+1) * mini_batch_size, :]
        mini_batch_Y = shuffled_Y[k * mini_batch_size : (k+1) * mini_batch_size]
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
    
    # Handling the end case (last mini-batch < mini_batch_size)
    if m % mini_batch_size != 0:
        mini_batch_X =shuffled_X[num_complete_minibatches * mini_batch_size : m, :]
        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m]
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
    
    
    return mini_batches

def step_one(x, y, batch_size, num_epochs, num_classes, learning_rate, device, seed, verbose = False):
    
    # use B/2 - 1 data to train the model, split the input data
    new_y = []
    for i in range(y.shape[0]):
        print('Obs ', i)
        
        # Loss and optimizer
        model = ConvNet(num_classes).to(device)
        criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
        # subset the data
        xn_train = np.delete(x, i, 0)
        yn_train = np.delete(y, i)
        
        # Train the model should I reload the model each loop???
        minibatches = random_mini_batches(xn_train, yn_train, batch_size, seed=seed)
        cost = []
        for epoch in range(num_epochs):
            for minibatch in minibatches:
                (items, labels) = minibatch
                items = items.reshape((items.shape[0], 1, items.shape[1]))
                items = torch.from_numpy(items).float().to(device)
                labels = torch.from_numpy(labels).long().to(device)
                
                # Forward pass
                outputs = model(items)
                loss = criterion(outputs, labels)

                # Backward and optimize
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                # get the prediction for the left point i
                if epoch % 50 == 0:
                    cost.append(loss.tolist())
                    print ('Epoch [{}/{}], Loss: {:.4f}' 
                           .format(epoch+1, num_epochs, loss.item()))
        
        # plot the cost
        if(verbose):
            plt.plot(cost)
            plt.ylabel('cost')
            plt.xlabel('epochs')
            plt.title("Learning rate = " + str(learning_rate))
            plt.show()
        
        # Predict
        y_test = y[i]
        tmp = numpy.zeros(num_classes)
        tmp[y_test] = 1
        y_test = tmp
#         if(y_test == 0):
            
#             y_test = np.array([1, 0])
#         else:
#             y_test = np.array([0, 1])
        x_test = x[i, :]
        x_test = torch.from_numpy(x_test).float().to(device)
        x_test = x_test.reshape((1, 1, x_test.shape[0]))
        outputs = model(x_test)
        prob = F.softmax(outputs, dim=1)
        # compute a new response (l2 norm)
        
        new_y.append(LA.norm(prob.detach().cpu().numpy() - y_test))
        
        if(verbose):
            _, predicted = torch.max(outputs.data, 1)
            pre = predicted.cpu().numpy()
            print("class:", pre)
            
    new_y = np.array(new_y)
        
    return new_y

def find_data(xn_train, new_y, yn_train, xn_test, yn_test, learning_rate, 
              num_epochs, batch_size, device, num_classes, seed, B2):
    """
    B2: number of data chosed for the next iteration
    """
    
    # Loss and optimizer
    model = ConvNet(num_classes).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    # Train the model
    minibatches = random_mini_batches(xn_train, new_y, batch_size, seed=seed)

    for epoch in range(num_epochs):
        for minibatch in minibatches:
            (items, labels) = minibatch

            items = items.reshape((items.shape[0], 1, items.shape[1]))
            items = torch.from_numpy(items).float().to(device)
            labels = torch.from_numpy(labels).long().to(device)

            # Forward pass
            outputs = model(items)
            optimizer.zero_grad()
            loss = criterion(outputs, labels)

            # Backward and optimize
            loss.backward()
            optimizer.step()
            
    # Now use the n-B/2 data to predict
    xn_test = torch.from_numpy(xn_test).float().to(device)
    xn_test = xn_test.reshape((xn_test.shape[0], 1, xn_test.shape[1]))
    outputs = model(xn_test)
    probs = F.softmax(outputs, dim=1)

    # remake yn_test
    y_ec = []
    label_encoder = LabelEncoder()
    integer_encoded = label_encoder.fit_transform(yn_test)
    onehot_encoder = OneHotEncoder(sparse=False)
    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
    y_ec = onehot_encoder.fit_transform(integer_encoded)
#     for y in yn_test:
#         if(y == 0):
#             y_ec.append([1, 0])
#         else:
#             y_ec.append([0, 1])
    y_ec = np.array(y_ec)    
    resid = probs.detach().cpu().numpy() - y_ec
    new_y = []
    for res in resid:
        new_y.append(LA.norm(res))
    new_y = np.array(new_y)   
    new_y  = new_y/new_y.sum()

    # sample b/2 points propotional to new_y, why do we choose the ones with worst pred
    draw = choice(range(xn_test.shape[0]), B2, p = new_y)

    # combine drawed points and previous xn_train
    xn_train = xn_train.reshape((xn_train.shape[0], 1, xn_train.shape[1]))
    xn_train = torch.from_numpy(xn_train).float().to(device)
    yn_train = torch.from_numpy(yn_train).long().to(device)
    yn_test = torch.from_numpy(yn_test).long().to(device)
    x = torch.cat([xn_train, xn_test[draw, :, :]], dim=0)
    y = torch.cat([yn_train, yn_test[draw]])
    
    ## also return the left training set
    yn_test = np.delete(yn_test, draw)
    xn_test = np.delete(xn_test, draw, 0)
    
    return (x, y, yn_test, xn_test)

def compute_loss(x, y, num_classes, device, learning_rate, num_epochs = 1000, mini_batch_size = 128, seed=0):
    
    minibatches = random_mini_batches(x, y, mini_batch_size, seed)
    cost = []
    # Loss and optimizer
    model = ConvNet(num_classes).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    
    for epoch in range(num_epochs):
        for minibatch in minibatches:
            (items, labels) = minibatch
            # Forward pass
            outputs = model(items)
            optimizer.zero_grad()
            loss = criterion(outputs, labels)

            # Backward and optimize
            loss.backward()
            optimizer.step()

            if epoch % 100 == 0:
                    cost.append(loss.tolist())
                    print ('Epoch [{}/{}], Loss: {:.4f}' 
                           .format(epoch+1, num_epochs, loss.item()))
                    
    return cost[-1]

# active learning

def ac(x_data, y_data, device, test_size = 0.33, seed = 0, prop = 0.5, B0_p = 0.5, num_epochs2 = 1000, 
       num_epochs1 = 200, num_classes = 2, batch_size1 = 64, learning_rate = 0.001, 
       batch_size2 = 128, iteration = 1, verbose = False):
    
    """
    prop: B propotion of the original data
    B0_p: B0 propotion of B
    """
    
    np.random.seed(seed)
    loss = []
    for i in range(iteration):
        # Random split the data to traning (n * p) and testing (m * p)
        x_train, x_test, y_train, y_test = train_test_split(
            x_data, y_data, test_size = test_size, random_state = seed)
        # At most B lables can be obtained
        B2 = int(x_train.shape[0] * prop * B0_p)
        # - random sample B/2 from n traning points
        xn_train, xn_test, yn_train, yn_test = train_test_split(
            x_train, y_train, test_size = 1 - prop * B0_p, random_state = seed)
        # get the new y and train the model again
        new_y = step_one(xn_train, yn_train, batch_size1, num_epochs1,
                          num_classes, learning_rate, device, seed, verbose)

        x, y, _, _ = find_data(xn_train, new_y, yn_train, xn_test, yn_test, learning_rate, 
                  num_epochs1, batch_size1, device, num_classes, seed, B2)

        cost = compute_loss(x, y, num_classes, device, learning_rate, num_epochs2, batch_size2, seed)
    
        loss.append(cost)
                    
    return loss

# plt.plot(cost)
# plt.ylabel('cost')
# plt.xlabel('epochs')
# plt.title("Learning rate = " + str(learning_rate))
# plt.show()

# ramdonly sample B then compute loss
def rand_cnn(x_data, y_data, device, test_size = 0.33, seed = 0, prop = 0.5, num_epochs = 1000,
      num_classes = 2, learning_rate = 0.001, mini_batch_size = 128, iteration = 1, verbose = False):
    
    np.random.seed(seed)
    v = []
    for i in range(iteration):
        
        # split training and testing
        x_train, x_test, y_train, y_test = train_test_split(
        x_data, y_data, test_size = test_size, random_state = seed)
        
        # randomly sample B points from training
        xn_train, xn_test, yn_train, yn_test = train_test_split(
            x_train, y_train, test_size = 1 - prop, random_state = seed)

        # train the nn and get loss
        minibatches = random_mini_batches(xn_train, yn_train, mini_batch_size, seed)
        cost = []
        # Loss and optimizer
        model = ConvNet(num_classes).to(device)
        criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

        for epoch in range(num_epochs):
            for minibatch in minibatches:
                (items, labels) = minibatch
                
                items = items.reshape((items.shape[0], 1, items.shape[1]))
                items = torch.from_numpy(items).float().to(device)
                labels = torch.from_numpy(labels).long().to(device)
                
                # Forward pass
                outputs = model(items)
                optimizer.zero_grad()
                loss = criterion(outputs, labels)

                # Backward and optimize
                loss.backward()
                optimizer.step()

                if epoch % 100 == 0:
                        cost.append(loss.tolist())
                        print ('Epoch [{}/{}], Loss: {:.4f}' 
                               .format(epoch+1, num_epochs, loss.item()))
                        
        v.append(cost[-1])
                    
    return v

# read the data
f = open('wdbc.data')

x_data = []
y_data = []

for line in f:
    data_line = line.rstrip().split(',')
    x_data.append([float(i) for i in data_line[2:]])
    label = []
    if(data_line[1] == 'M'):
        label = 1
    else:
        label = 0
    y_data.append(label)
    
x_data = np.array(x_data)
y_data = np.array(y_data)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

rand = rand_cnn(x_data, y_data, device, iteration = 50)

al = ac(x_data, y_data, device, iteration = 50)

res = [rand, al] 
  
fig = plt.figure(figsize =(5, 5)) 
  
# Creating axes instance 
ax = fig.add_axes([0, 0, 1, 1]) 
  
# Creating plot 
bp = ax.boxplot(res) 
ax.set_ylabel('Loss')
plt.xticks([1, 2], ['Random', 'Active learning'])
# show plot 
plt.savefig('comparison.jpg', bbox_inches='tight', dpi=350)
plt.show()

test_size = 0.33
m = 2
prop=0.5
B0_p=0.5

####new approach
x_train, x_test, y_train, y_test = train_test_split(
            x_data, y_data, test_size = test_size)
B2 = int(x_train.shape[0] * prop * B0_p)
# - random sample B/2 from n traning points
xn_train, xn_train2, yn_train, yn_train2 = train_test_split(
    x_train, y_train, test_size = 1 - prop * B0_p)

####bootstrap k times and keep track the sample id in the k bootstrapped data
### sample b form B-B0
## build a hash table indicating the samples without the datapoint i
def bootstrap(limit, b, k, seed):
    """
    out_id: data id for training in each bst sample
    ##hash_tb: data id -> bst sample that not in
    """
    full = np.array(range(limit))
    np.random.seed(seed)
    hash_tb = dict()
    out_id = []
    for i in range(k):
        pickups = np.array(np.random.randint(0, limit, size = b))
        out_id.append(pickups)
#         complement = np.setdiff1d(full, pickups)
#         for key in complement:
#             if(key in hash_tb):
#                 hash_tb[key].append(i)
#             else:
#                 hash_tb[key] = []
     
    return out_id

boot_nn(xn_train2, yn_train2, b = 200, seed = 0, device = device, learning_rate = 0.001, batch_size = 64, 
            num_epochs = 300, num_classes = 2, k = 50)

## bootstrap with replacement, boot sample size = 
def boot_nn(xn_train2, yn_train2, seed, device, learning_rate, batch_size, 
            num_epochs, num_classes = 2, k = 50):
    limit = xn_train2.shape[0]
    out_id = bootstrap(limit, limit, k, seed)
    
    ## there should be multiple predictions for all the data in the pool for bootstrapping
    full = np.array(range(limit))
    pool_pred = [None] * limit
    # one hot encoding 
    label_encoder = LabelEncoder()
    integer_encoded = label_encoder.fit_transform(yn_train2)
    onehot_encoder = OneHotEncoder(sparse=False)
    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
    y_vec = onehot_encoder.fit_transform(integer_encoded)
    
    ## train nn for all bootstrapped sample
    for i in range(k):
        ids = out_id[i]
        
        ## get data
        x = xn_train2[ids, :]
        y = yn_train2[ids]
        
        # Loss and optimizer
        model = ConvNet(num_classes).to(device)
        criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
        
        # train the model
        minibatches = random_mini_batches(x, y, batch_size, seed)
        cost = []
        for epoch in range(num_epochs):
            for minibatch in minibatches:
                (items, labels) = minibatch
                items = items.reshape((items.shape[0], 1, items.shape[1]))
                items = torch.from_numpy(items).float().to(device)
                labels = torch.from_numpy(labels).long().to(device)
                
                # Forward pass
                outputs = model(items)
                loss = criterion(outputs, labels)

                # Backward and optimize
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                # get the prediction for the left point i
                if epoch % 100 == 0:
                    cost.append(loss.tolist())
                    print ('Epoch [{}/{}], Loss: {:.4f}' 
                           .format(epoch+1, num_epochs, loss.item()))
        # predict 
        complement = np.setdiff1d(full, ids)
        x_test =  np.delete(xn_train2, ids, 0)
        x_test = torch.from_numpy(x_test).float().to(device)
        x_test = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))
        outputs = model(x_test)
        probs = F.softmax(outputs, dim=1)
        probs = probs.detach().cpu().tolist()
        for i in range(len(complement)):
            if pool_pred[complement[i]] is None:
                pool_pred[complement[i]] = []
            pool_pred[complement[i]].append(probs[i])
    
    i = 0
    resid = []
    ## average across the pred prob
    for pred in pool_pred:
        if pred is None:
            resid.append(yn_train2[i])
            i += 1
            continue
        pred = np.array(pred)
        pread_mean = np.mean(pred, axis=0)
        # normalize the estimation
        pread_mean = pread_mean / np.sum(pread_mean)
        res = y_vec[i] - pread_mean
        resid.append(res)
        i += 1

    new_y = []
    for res in resid:
        new_y.append(LA.norm(res))
        
    return new_y

#### active learning with bootstrap sampling
def bt_al(x_data, y_data, device, m, test_size = 0.33, seed = 0, prop = 0.5, B0_p = 0.5, num_epochs = 500,
          num_classes = 2, learning_rate = 0.001, mini_batch_size = 64, iteration = 1, bt_samples = 50,
          num_epochs2 = 1000, batch_size2 = 128, verbose = False):
    
    np.random.seed(seed)
    loss = []
    for i in range(iteration):
        x_train, x_test, y_train, y_test = train_test_split(
                    x_data, y_data, test_size = test_size, random_state = seed)
        # - random sample B2 from n traning points
        xn_train, xn_test, yn_train, yn_test = train_test_split(
            x_train, y_train, test_size = 1 - prop * B0_p, random_state = seed)
        b = math.ceil(x_train.shape[0] * prop * (1 - B0_p) / m)

    # Use B2 for bootsrtap sampling, then get new y, then predict on n-B2, select b; then B2 + b for training, 
    # sampling, then repeat, until m times

        for i in m + 1:
            new_y = boot_nn(xn_train, yn_train, seed = seed, device = device, learning_rate = learning_rate, 
                    batch_size = mini_batch_size, num_epochs = num_epochs, num_classes = num_classes, k = bt_samples)
            new_y = np.array(new_y)
            xn_train, yn_train, xn_test, yn_test = find_data(xn_train, new_y, yn_train, xn_test, yn_test, learning_rate, 
                            num_epochs, batch_size, device, num_classes, seed, b)

        cost = compute_loss(xn_train, yn_train, num_classes, device, learning_rate, num_epochs2, batch_size2, seed)
        
    loss.append(cost)
                    
    return loss

bst_al = bt_al(x_data, y_data, device, m = 3, test_size = 0.33, seed = 0, prop = 0.5, B0_p = 0.5, num_epochs = 500,
          num_classes = 2, learning_rate = 0.001, mini_batch_size = 64, iteration = 1, bt_samples = 50,
          num_epochs2 = 1000, batch_size2 = 128, iteration = 1, verbose = False)